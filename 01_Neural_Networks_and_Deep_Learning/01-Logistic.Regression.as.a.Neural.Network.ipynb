{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- [Binary Classification](#bincls)\n",
    "\t- Use logistic regression algorithm for binary classification\n",
    "- [Logistic Regresssion](#logreg)\n",
    "- [Gradient Decent](#gradecent)\n",
    "- [Derivatives](#deriv)\n",
    "- [Computation Graph](#compgraph)\n",
    "- [Logistic Regression Gradient Descent](#logreg-gradecent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "\n",
    "In a binary classificatin problem, the result is a discrete value output.\n",
    "\n",
    "- The goal is to train a classifier that the input is an image represented by a feature vector, ùë•, and predicts whether the corresponding label ùë¶ is 1 or 0.\n",
    "- Process m training sets w/o loops using matrix\n",
    "- Computation of NN : Forward & Backward propagation steps\n",
    "\n",
    "$X^{(i)} \\in \\mathbb{R}^{n_{x}}$ ($i^{th}$ training example)<br>\n",
    "$Y^{(i)} \\in \\{0,1\\}$ where `i=1,...,m` <br>\n",
    "\n",
    "Put an entire training set $\\{(x^{(1)},y^{(1)}), ... , (x^{(m)},y^{(m)})\\} $ into $X$:<br>\n",
    "$X=$\n",
    "\t\\begin{bmatrix}\n",
    "\t| & | & & |\\\\\n",
    "\tX^{(1)} & X^{(2)} & ... & X^{(m)}\\\\\n",
    "\t| & | & & |\\\\\n",
    "\t\\end{bmatrix} $\\in \\mathbb{R}^{n_{x}\\times m}$<br>\n",
    "$Y=$\n",
    "\t\\begin{bmatrix}\n",
    "\tY^{(1)} & X^{(2)} & ... & Y^{(m)}\\\\\n",
    "\t\\end{bmatrix} $\\in \\mathbb{R}^{1\\times m}$\n",
    "\n",
    "`X.shape = ` $(n_{x},m)$<br>\n",
    "`Y.shape = ` $(1,m)$\n",
    "\n",
    "$n_{x}$ = (number of input features) = 64x64x3 = 12,288<br>\n",
    "$m_{train}$ = (number of training data)<br>\n",
    "$m_{test}$ = (number of test data)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression\n",
    "\n",
    "- Logistic regression is a learning algorithm used in a supervised learning problem when the output ùë¶ are all either zero or one. The goal of logistic regression is to minimize the error between its predictions and training data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img id=\"logreg\" src=\"https://i.imgur.com/9iAacgB.png\" style=\"width:550px;height:320px;\">\n",
    "\n",
    "In linear regression, $\\hat{y} = w^{T}X+b$. But $\\hat{y}$ should be $0\\leq\\hat{y}\\leq1$<br>\n",
    "In logistic regression,\n",
    "$\\hat{y} = \\sigma({w^T+b})$ using sigmoid function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\hat{y}= P(y=1|x) = \\sigma(w^T x+b) = \\sigma(z)= \\frac{1}{1+e^{-z}}$ <br>\n",
    "\n",
    "for `i=1,...m` $\\hat{y}^{(i)}= \\sigma(w^T x^{(i)}+b) = \\sigma(z^{(i)})= \\frac{1}{1+e^{-z^{(i)}}}$<br>\n",
    "given $\\{(x^{(1)}, y^{(1)}), ... (x^{(m)}, y^{(m)})\\}$, want $\\hat{y}^{(i)} \\approx y^{(i)}$\n",
    "\n",
    "To train the parameters $\\omega$ and b, we need to define a Cost function\n",
    "- Loss $L(\\hat{y},y)= -(y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})})$\n",
    "\n",
    "- Cost $J(\\omega, b)= \\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)},y^{(i)})= -\\frac{1}{m} \\sum_{i=1}^{m}[y^{(i)}\\log{\\hat{y}^{(i)}}+(1-y^{(i)})\\log{(1-\\hat{y}^{(i)})}]$\n",
    "\n",
    "\n",
    "In logistic regresion implementation objective is to find parameters $w$ and $b$ to make $\\hat{y}$ a good estimate of the chance of Y being equal to 1.\n",
    "\n",
    "\n",
    "In training logistic regression model, find $w$ and $b$ that minimize overall cost function, $J(w,b)$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img id=\"gradecent\" src=\"https://i.imgur.com/m5HTDzl.png\" style=\"width:550px;height:300px;\">\n",
    "\n",
    "- You've seen the Logistic regression model. Loss function that measures how well you're doing on the single training example. Cost function that measures how well your parameters $w$ and $b$ are doing on your entire training set.\n",
    "- Now let's talk about how you can use the gradient descent algorithm to train, or to learn, the parameters w and b on your training set.\n",
    "\n",
    "- Take iterative steps from initial position to the global optimum\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/2mGFlWu.png\" style=\"width:550px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/XSPppwf.png\" style=\"width:550px;height:300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/fTmw5jJ.png\" style=\"width:550px;height:300px;\">\n",
    "\n",
    "One step of backward propagation on a computation graph yields derivative of final output variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression Gradient descent\n",
    "\n",
    "Want: modify w and b to reduce L-> go backwards propagation to compute derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://i.imgur.com/Ntnnqey.png\" style=\"width:550px;height:300px; float: left;\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://i.imgur.com/AhMybZm.png\" style=\"width:550px;height:250px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://i.imgur.com/2pseXrt.png\" style=\"width:550px;height:280px; float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Type the following in 'Python Console' to install dependencies in jupyter\n",
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} numpy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [data types](https://docs.scipy.org/doc/numpy-1.10.1/user/basics.types.html)\n",
    "\n",
    "# 64x64 with 8-bit integer components\n",
    "print(np.iinfo(np.uint8))\n",
    "\n",
    "for index in range(train_set_x_orig.shape[0]):\n",
    "    cat = train_set_x_orig[index]\n",
    "    assert(np.ndarray(shape=(64, 64, 3), dtype=np.uint8).shape == cat.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [data types](https://docs.scipy.org/doc/numpy-1.10.1/user/basics.types.html)\n",
    "\n",
    "# 64x64 with 8-bit integer components\n",
    "print(np.iinfo(np.uint8))\n",
    "\n",
    "for index in range(train_set_x_orig.shape[0]):\n",
    "    cat = train_set_x_orig[index]\n",
    "    assert(np.ndarray(shape=(64, 64, 3), dtype=np.uint8).shape == cat.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [data types](https://docs.scipy.org/doc/numpy-1.10.1/user/basics.types.html)\n",
    "\n",
    "# 64x64 with 8-bit integer components\n",
    "print(np.iinfo(np.uint8))\n",
    "\n",
    "for index in range(train_set_x_orig.shape[0]):\n",
    "    cat = train_set_x_orig[index]\n",
    "    assert(np.ndarray(shape=(64, 64, 3), dtype=np.uint8).shape == cat.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [data types](https://docs.scipy.org/doc/numpy-1.10.1/user/basics.types.html)\n",
    "\n",
    "# 64x64 with 8-bit integer components\n",
    "print(np.iinfo(np.uint8))\n",
    "\n",
    "for index in range(train_set_x_orig.shape[0]):\n",
    "    cat = train_set_x_orig[index]\n",
    "    assert(np.ndarray(shape=(64, 64, 3), dtype=np.uint8).shape == cat.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-ac259a959011>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# 64x64 with 8-bit integer components\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miinfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muint8\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mindex\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_set_x_orig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# [data types](https://docs.scipy.org/doc/numpy-1.10.1/user/basics.types.html)\n",
    "\n",
    "# 64x64 with 8-bit integer components\n",
    "print(np.iinfo(np.uint8))\n",
    "\n",
    "for index in range(train_set_x_orig.shape[0]):\n",
    "    cat = train_set_x_orig[index]\n",
    "    assert(np.ndarray(shape=(64, 64, 3), dtype=np.uint8).shape == cat.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-4c76f86f",
   "language": "python",
   "display_name": "PyCharm (PYTHON_LOCAL)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}