{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Binary Classification](#bincls)\n",
    "\t- Use logistic regresssion algorithm for binary classification\n",
    "- [Logistic Regresssion](#logreg)\n",
    "- [Gradient Decent](#gradecent)\n",
    "- [Derivatives](#deriv)\n",
    "- [Computation Graph](#compgraph)\n",
    "- [Logistic Regression Gradient Descent](#logreg-gradecent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- process m training sets w/o loops (using matrix)\n",
    "- computation of NN : forward & backward propagation steps\n",
    "- 'Logistic Regression' to convery the above ideas (algorithm for binary classification)\n",
    "\n",
    "<img src=\"https://i.imgur.com/w1cixJF.png\" style=\"width:550px;height:350px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://i.imgur.com/TkGyzQ9.pngK\" style=\"width:550px;height:350px; float: left;\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unroll $i^{th}$ training example into $X^{(i)}$ (three 64 by 64 matrices of RGB)<br>\n",
    "Put entire training sets into $ X \\in$ $R^{n_{x} \\times m}$<br>\n",
    "$X.shape=(n_{x}, m)$<br>\n",
    "$Y.shape=(1, m)$\n",
    "\n",
    "\n",
    "$n_{x} =$ (number of input features) $= 64\\times64\\times3 = 12,288$<br>\n",
    "$m_{train} =$ (number of training data)<br>\n",
    "$m_{test} =$ (number of test data)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img id=\"logreg\" src=\"https://i.imgur.com/9iAacgB.png\" style=\"width:550px;height:320px; float: left;\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://i.imgur.com/ae4mzIU.png\" style=\"width:570px;height:330px; float: left;\">\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression,\n",
    "$\\hat{y} = w^T+b$. But $\\hat{y}$ should be $0\\leq\\hat{y}\\leq1$<br>\n",
    "$\\hat{y} = \\sigma({w^T+b})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regresion implementation objective is to find parameters $w$ and $b$ to make $\\hat{y}$ a good estimate of the chance of Y being equal to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training logistic regression model, find $w$ and $b$ that minimize overall cost function, $J(w,b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y}^{(i)}= \\sigma(w^T X^{(i)} + b) = \\sigma(z^{(i)})$$\n",
    "\n",
    "$$z^{(i)}= w^T X^{(i)} + b$$\n",
    "\n",
    "$$Given\\ \\{(x^{(1)}, y^{(1)}), ... (x^{(m)}, y^{(m)})\\},\\ want\\ \\hat{y}^{(i)} \\approx y^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You've seen the logistic regression model. You've seen the loss function that measures how well you're doing on the single training example. You've also seen the cost function that measures how well your parameters w and b are doing on your entire training set.\n",
    "- Now let's talk about how you can use the gradient descent algorithm to train, or to learn, the parameters w and b on your training set.\n",
    "- Take iterative steps from initial position to the global optimum\n",
    "\n",
    "<img id=\"gradecent\" src=\"https://i.imgur.com/m5HTDzl.png\" style=\"width:550px;height:300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/2mGFlWu.png\" style=\"width:550px;height:300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img id=\"deriv\" src=\"https://i.imgur.com/ffqllqb.png\" style=\"width:550px;height:280px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/HlVVp1t.png\" style=\"width:550px;height:280px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/DrdyBeg.png\" style=\"width:550px;height:280px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img id=\"compgraph\" src=\"https://i.imgur.com/zEX9fxz.png\" style=\"width:550px;height:300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/XSPppwf.png\" style=\"width:550px;height:300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/fTmw5jJ.png\" style=\"width:550px;height:300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img id=\"logreg-gradecent\" src=\"https://i.imgur.com/jKcH9jB.png\" style=\"width:550px;height:300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/Ntnnqey.png\" style=\"width:550px;height:300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dz = \\frac{dL}{da} \\cdot \\frac{da}{dz} = \\{-\\frac{y}{a} + \\frac{1-y}{1-a}\\}\\cdot\\{a\\cdot(1-a)\\}$$<br>\n",
    "$$\\frac{da}{dz} = \\frac{d\\sigma(z)}{dz} = \\frac{d}{dz}(\\frac{1}{1+e^{-z}}) = a\\cdot(1-a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/AhMybZm.png\" style=\"width:550px;height:250px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n_x = 2,\\ m = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/2pseXrt.png\" style=\"width:550px;height:280px; float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Type the following in 'Python Console' to install dependencies in jupyter\n",
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} numpy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [data types](https://docs.scipy.org/doc/numpy-1.10.1/user/basics.types.html)\n",
    "\n",
    "# 64x64 with 8-bit integer components\n",
    "print(np.iinfo(np.uint8))\n",
    "\n",
    "for index in range(train_set_x_orig.shape[0]):\n",
    "    cat = train_set_x_orig[index]\n",
    "    assert(np.ndarray(shape=(64, 64, 3), dtype=np.uint8).shape == cat.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-4c76f86f",
   "language": "python",
   "display_name": "PyCharm (PYTHON_LOCAL)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}